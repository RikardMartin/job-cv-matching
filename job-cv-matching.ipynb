{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Large Language Models to match CV documents to job postings\n",
    "See `README.md` for more background and information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "I assume you have `conda` installed (but any virtual environment with `pip` installed will do). For conda, open a terminal and type the following commands:\n",
    "```bash\n",
    "conda create -n job-cv-matching python=3.9\n",
    "conda activate job-cv-matching\n",
    "```\n",
    "\n",
    "We create the necessary environment from the `requirements` file. In the terminal, type:\n",
    "```bash\n",
    "pip install -r requirements\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we import the packages and set some parameters\n",
    "# When running this cell, make sure to select the job-cv-matching kernel from the virtual environment that we just created. If it does not show up, restart the jupyter server and try again.\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "base_path = 'data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the CV documents\n",
    "In this example, we will use a public dataset available on [Kaggle](https://www.kaggle.com). You will need an account and an API-key to connect and download the data with the method in this notebook. You will find info on how to set this up on [this link](https://github.com/Kaggle/kaggle-api).\n",
    "If you implement this on your own data, you just have to replace the call to the Kaggle-API with a call to your own source of CV documents, and then process the dataset accordingly to fit the format used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading curriculum-vitae.zip to ./data\n",
      " 72%|███████████████████████████▎          | 3.00M/4.18M [00:00<00:00, 4.68MB/s]\n",
      "100%|██████████████████████████████████████| 4.18M/4.18M [00:00<00:00, 5.21MB/s]\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "\n",
    "# Download the dataset of CV documents\n",
    "!mkdir data\n",
    "!kaggle datasets download leenardeshmukh/curriculum-vitae -p ./data --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11019</th>\n",
       "      <td>DotNet Developer</td>\n",
       "      <td>Technical Skills â¢ Languages: C#, ASP .NET M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>DotNet Developer</td>\n",
       "      <td>Education Details \\r\\nJanuary 2014  Education ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11021</th>\n",
       "      <td>DotNet Developer</td>\n",
       "      <td>Technologies ASP.NET, MVC 3.0/4.0/5.0, Unit Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11022</th>\n",
       "      <td>DotNet Developer</td>\n",
       "      <td>Technical Skills CATEGORY SKILLS Language C, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11023</th>\n",
       "      <td>DotNet Developer</td>\n",
       "      <td>TECHNICAL SKILLS â Programming Languages: C#...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11024 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Category                                                 cv\n",
       "0          Data Science  Skills * Programming Languages: Python (pandas...\n",
       "1          Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
       "2          Data Science  Areas of Interest Deep Learning, Control Syste...\n",
       "3          Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
       "4          Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n",
       "...                 ...                                                ...\n",
       "11019  DotNet Developer  Technical Skills â¢ Languages: C#, ASP .NET M...\n",
       "11020  DotNet Developer  Education Details \\r\\nJanuary 2014  Education ...\n",
       "11021  DotNet Developer  Technologies ASP.NET, MVC 3.0/4.0/5.0, Unit Te...\n",
       "11022  DotNet Developer  Technical Skills CATEGORY SKILLS Language C, C...\n",
       "11023  DotNet Developer  TECHNICAL SKILLS â Programming Languages: C#...\n",
       "\n",
       "[11024 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and print a few lines\n",
    "base_path = 'data/'\n",
    "raw = pd.read_csv(base_path+'Curriculum Vitae.csv')\n",
    "raw.rename(columns={'Resume': 'cv'}, inplace=True)\n",
    "raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarize CV documents\n",
    "CV documents are rather lengthy and often contains repeated information and are formatted as a selling text. Some of that text can act a a disturbing noise for the LLM-models that will interpret and transform the data. Since these models don't care about how nicely and well formatted the document is, we can summarize the documents to make them as information dense and to-the-point as possible. By doing so we also decrease the length, making them easier and cheaper for the GPT-based models to process.\n",
    "\n",
    "Any suitable LLM will do here, but i have chosen GPT-based models from [openai](https://openai.com), hosted on Microsoft [Azure](https://portal.azure.com). See `README.md` pre-requisites section for more information on how to set this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Sätt parameters for Azure openai\n",
    "openai_rg_name = 'openai-lab'\n",
    "openai_svc_name = 'openai-lab-rm'\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Choose your openai endpoint and key that you acquire when setting up Azure openai. I have set them as environment variables by using a .env-file.\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the model for summarizing CV documents. I have used an old name, but the model is in fact gpt-35-turbo.\n",
    "text_summarization_model = \"text-davinci-003\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-cv-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
